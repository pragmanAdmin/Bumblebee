{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "# Connect to MongoDB and load data\n",
    "client = MongoClient(\"use client link\")\n",
    "db = client.bumblebee\n",
    "\n",
    "train_data = pd.DataFrame(list(db.attribute_train_data.find()))\n",
    "val_data = pd.DataFrame(list(db.attribute_val_data.find()))\n",
    "train_solution = pd.DataFrame(list(db.attribute_train_solution.find()))\n",
    "val_solution = pd.DataFrame(list(db.attribute_val_solution.find()))\n",
    "\n",
    "# Merge data with solutions\n",
    "train_merged = pd.merge(train_data, train_solution, on='indoml_id')\n",
    "val_merged = pd.merge(val_data, val_solution, on='indoml_id')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_merged = train_merged.drop(columns=['indoml_id'])\n",
    "val_merged = val_merged.drop(columns=['indoml_id'])\n",
    "\n",
    "# Combine the category levels into one string for tokenization\n",
    "for df in [train_merged, val_merged]:\n",
    "    df['category'] = df[['L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']].apply(lambda x: ' '.join(x.dropna()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.title = dataframe['title']\n",
    "        self.category = dataframe['category']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        category = str(self.category[index])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'title_text': title,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.tokenizer.encode(category, add_special_tokens=True, max_length=self.max_len, truncation=True), dtype=torch.long).flatten()\n",
    "        }\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Creating datasets\n",
    "train_dataset = CustomDataset(train_merged, tokenizer, MAX_LEN)\n",
    "val_dataset = CustomDataset(val_merged, tokenizer, MAX_LEN)\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transformer Model Definition\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(dropout_output)\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel(num_labels=len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train_model(model, dataloader, optimizer, device):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1).flatten()\n",
    "            accuracy = (preds == labels).cpu().numpy().mean()\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader)\n",
    "    \n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    \n",
    "    train_loss = train_model(model, train_dataloader, optimizer, device)\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    \n",
    "    val_loss, val_accuracy = evaluate_model(model, val_dataloader, device)\n",
    "    print(f'Validation loss: {val_loss}, Validation accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "\n",
    "MODEL_PATH = 'path/to/saved/model.pth'\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "# Load the trained model for inference\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# Dummy input, replace with actual data\n",
    "input_text = \"product description\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_encoding = tokenizer.encode_plus(\n",
    "    input_text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "input_ids = input_encoding['input_ids'].to(device)\n",
    "attention_mask = input_encoding['attention_mask'].to(device)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, attention_mask)\n",
    "    prediction = torch.argmax(output, dim=1).cpu().item()\n",
    "\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
